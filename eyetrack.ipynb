{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wnsdh\\Desktop\\Lab\\vision\\L2CS-Net-modified\\l2cs\\pipeline.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.model.load_state_dict(torch.load(self.weights, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/wnsdh/Downloads/snuwet_test\\webcam_record_250417-2235.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "검출률: 99.8%:   1%|▏         | 543/38333 [00:07<08:20, 75.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m     frame_batch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(frame_buffer)\n\u001b[1;32m---> 50\u001b[0m     results_batch \u001b[38;5;241m=\u001b[39m \u001b[43mgaze_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m timestamp, results \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(timestamp_buffer, results_batch):\n\u001b[0;32m     52\u001b[0m         gazeData\u001b[38;5;241m.\u001b[39mappend((timestamp, results))\n",
      "File \u001b[1;32mc:\\Users\\wnsdh\\Desktop\\Lab\\vision\\L2CS-Net-modified\\l2cs\\pipeline.py:169\u001b[0m, in \u001b[0;36mPipeline.step_batch\u001b[1;34m(self, frames)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m frames_list:\n\u001b[0;32m    166\u001b[0m      \u001b[38;5;66;03m# Process each frame individually using the step method\u001b[39;00m\n\u001b[0;32m    167\u001b[0m      \u001b[38;5;66;03m# Ensure frame is a valid image (3D numpy array)\u001b[39;00m\n\u001b[0;32m    168\u001b[0m      \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(frame, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m frame\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m--> 169\u001b[0m          result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m          batch_results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[0;32m    171\u001b[0m      \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    172\u001b[0m           \u001b[38;5;66;03m# Handle potential invalid frames in the batch if necessary\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wnsdh\\Desktop\\Lab\\vision\\L2CS-Net-modified\\l2cs\\pipeline.py:61\u001b[0m, in \u001b[0;36mPipeline.step\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m     58\u001b[0m frame_height, frame_width, _ \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_detector:\n\u001b[1;32m---> 61\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_detection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mdetections:\n\u001b[0;32m     64\u001b[0m         max_area \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\mediapipe\\python\\solutions\\face_detection.py:105\u001b[0m, in \u001b[0;36mFaceDetection.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m     91\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns a list of the detected face location data.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m    detected face location data.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\mediapipe\\python\\solution_base.py:372\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    366\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    368\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    369\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    370\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 372\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from l2cs import select_device, Pipeline, render\n",
    "import time, pickle\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "CWD = pathlib.Path.cwd()\n",
    "path = 'C:/Users/wnsdh/Downloads/snuwet_test'\n",
    "webcam_records = glob.glob(path + '/webcam_record_*.mp4')\n",
    "\n",
    "# 시선 추적을 위한 파이프라인 초기화\n",
    "gaze_pipeline = Pipeline(\n",
    "    weights=CWD / 'models' / 'L2CSNet_gaze360.pkl',\n",
    "    arch='ResNet50',\n",
    "    device=torch.device('cuda')\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 32  # 배치 크기 설정\n",
    "\n",
    "for webcam_record in webcam_records:\n",
    "    print(webcam_record)\n",
    "    \n",
    "    gazeData = []  # 시선 데이터를 저장할 리스트\n",
    "    frame_buffer = []  # 프레임을 모아둘 버퍼\n",
    "    timestamp_buffer = []  # 타임스탬프를 모아둘 버퍼\n",
    "    \n",
    "    cap = cv2.VideoCapture(webcam_record)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(range(total_frames))\n",
    "        for _ in pbar:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "                \n",
    "            timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000\n",
    "            frame_buffer.append(frame)\n",
    "            timestamp_buffer.append(timestamp)\n",
    "            \n",
    "            # 버퍼가 배치 크기만큼 찼거나 마지막 프레임일 때 처리\n",
    "            if len(frame_buffer) >= BATCH_SIZE or _ == total_frames - 1:\n",
    "                # 배치 처리\n",
    "                try:\n",
    "                    frame_batch = np.stack(frame_buffer)\n",
    "                    results_batch = gaze_pipeline.step_batch(frame_batch)\n",
    "                    for timestamp, results in zip(timestamp_buffer, results_batch):\n",
    "                        gazeData.append((timestamp, results))\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                \n",
    "                # 버퍼 초기화\n",
    "                frame_buffer = []\n",
    "                timestamp_buffer = []\n",
    "            \n",
    "            # 검출률 표시 업데이트\n",
    "            detection_rate = len([g for g in gazeData if g[1].pitch.size > 0]) / (len(gazeData)+1) * 100\n",
    "            pbar.set_description(f\"검출률: {detection_rate:.1f}%\")\n",
    "    \n",
    "    cap.release()\n",
    "    pickle.dump(gazeData, open(webcam_record.replace('.mp4', '.pkl').replace('webcam_record_', 'gaze_data_'), 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
